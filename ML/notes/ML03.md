# 资料

- https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.20222472.J_3678908510.6.275467c2PlUzzf&postId=170914



# 笔记

## 0. 流程总览

- Step1: 库函数导入
- Step2: 数据读取/载入
- Step3: 数据信息简单查看
- Step4: 可视化描述
- Step5: 利用 LightGBM 进行训练与预测
- Step6: 利用 LightGBM 进行特征选择
- Step7: 通过调整参数获得更好的效果



## 1. 导入函数库

```python
##  基础函数库
import numpy as np 
import pandas as pd

## 绘图函数库
import matplotlib.pyplot as plt
import seaborn as sns
```



## 2. 数据读取/载入

```python
df = pd.read_csv('./high_diamond_ranked_10min.csv')
y = df.blueWins
```



## 3. 数据信息简单查看

- info
- head、tail
- value_counts
- describe

```python
# df整体信息
df.info()
# 头or尾行
df.head()
df.tail()
# 统计信息
df.describe()
```

```python
# 标注特征列和标签列
drop_cols = ['gameId', 'blueWins']
x = df.drop(drop_cols, axis=1)
y = df.blueWins
```

```python
# 查看标签数量（是否均衡）
y.value_counts()
```

```python
# 去除冗余数据(由其他列通过简单运算可得出的列)
drop_cols = ['redFirstBlood','redKills','redDeaths','redGoldDiff','redExperienceDiff','blueCSPerMin','blueGoldPerMin','redCSPerMin','redGoldPerMin']
x.drop(drop_cols, axis=1, inplace=True)
```



## 4. 可视化描述

### 知识点

- 小提琴图: `sns.violinplot()`。用来展示多组数据的分布状态以及概率密度
- 热力图：`sns.heatmap()`。展示特征的相关性
- 散点图：`sns.swarmplot`。与热力图类似，数据点直接显示在图上（而不是概率密度）
- 平均值：df.mean()
- 标准差：df.std()
- 抽样：df.sample()
- Melt: pd.melt()。将df宽数据变成长数据。用例见散点图
- Concat: pd.concat()。拼合两df形成新df
- pd.hist()。频数直方图
- 标准分数(Standard Score) = （数据 - 平均值）/ 标准差，表示数据在一组数值内的相对位置
- Sis.pairplot()。分析特征两两组合后，结果的区分度

### 分析过程

1. 整体查看

```python
data = x
data_std = (data - data.mean()) / data.std()
# 将特征切分成两部分方便显示
data = pd.concat([y, data_std.iloc[:, 0:9]], axis=1)
data = pd.melt(data, id_vars='blueWins', var_name='Features', value_name='Values')

# 分成两张子图
fig, ax = plt.subplots(1,2,figsize=(15,5))

# 绘制小提琴图（0～9特征）
sns.violinplot(x='Features', y='Values', hue='blueWins', data=data, split=True,
               inner='quart', ax=ax[0], palette='Blues')
fig.autofmt_xdate(rotation=45)


# 继续plot第二部分
data = x
data_std = (data - data.mean()) / data.std()
data = pd.concat([y, data_std.iloc[:, 9:18]], axis=1)
data = pd.melt(data, id_vars='blueWins', var_name='Features', value_name='Values')

# 绘制小提琴图
sns.violinplot(x='Features', y='Values', hue='blueWins', 
               data=data, split=True, inner='quart', ax=ax[1], palette='Blues')
fig.autofmt_xdate(rotation=45)

plt.show()
```

从小提琴图大致可以得出结论：

- 击杀英雄数量越多更容易赢，死亡数量越多越容易输（bluekills与bluedeaths左右的区别）。
- 助攻数量与击杀英雄数量形成的图形状类似，说明他们对游戏结果的影响差不多。
- 一血的取得情况与获胜有正相关，但是相关性不如击杀英雄数量明显。
- 经济差与经验差对于游戏胜负的影响较大。
- 击杀野怪数量对游戏胜负的影响并不大。



2. 清理掉低效特征

- 相关度高的

```python
# 通过热力图展示特征相关性
plt.figure(figsize=(18,14))
sns.heatmap(round(x.corr(),2), cmap='Blues', annot=True)
plt.show()

# 去除冗余（相关性高）的特征
drop_cols = ['redAvgLevel','blueAvgLevel']
x.drop(drop_cols, axis=1, inplace=True)
```



- 针对：插眼数。分析插眼数差对胜负的影响

```python
sns.set(style='whitegrid', palette='muted')

# 构造两个新特征：插眼差，拔眼差
x['wardsPlacedDiff'] = x['blueWardsPlaced'] - x['redWardsPlaced']
x['wardsDestroyedDiff'] = x['blueWardsDestroyed'] - x['redWardsDestroyed']

# 取1000个样本，展示在散点图上
data = x[['blueWardsPlaces', 'blueWardsDestroyed', 'wardsPlacedDiff', 'wardsDestroyedDiff']].sample(1000)
data_std = (data - data.mean()) / data.std()
data = pd.concat([y, data_std], axis=1)
data = pd.melt(data, id_vars='blueWins', var_name="Features", value_name='Values')

plt.figure(figsize=(10,6))
sns.swarmplot(x='Features', y='Values', hue='blueWins', data=data)
plt.xticks(rotation=45)
plt.show()
```

看出擦眼数与胜负没有显著规律，去除这些特征

```python
drop_cols = ['blueWardsPlaced','blueWardsDestroyed','wardsPlacedDiff',
            'wardsDestroyedDiff','redWardsPlaced','redWardsDestroyed']
x.drop(drop_cols, axis=1, inplace=True)
```



- 针对：击杀、辅助击杀

```python
# 净击杀
x['killsDiff'] = x['blueKills'] - x['blueDeaths']
# 助攻差
x['assistsDiff'] = x['blueAssists'] - x['redAssists']

# 直方图
x[['blueKills','blueDeaths','blueAssists','killsDiff','assistsDiff','redAssists']].hist(figsize=(12,10), bins=20)
plt.show()
```

可看出，击杀、死亡、助攻数的分布差别不大

但是，净击杀和助攻差与原分布差别很大 =》 构造这两个新特征

```python
# 散点图验证一下
data = x[['blueKills','blueDeaths','blueAssists','killsDiff','assistsDiff','redAssists']].sample(1000)

data_std = (data - data.mean()) / data.std()
data = pd.concat([y, data_std], axis=1)
data = pd.melt(data, id_vars='blueWins', var_name='Features', value_name='Values')

plt.figure(figsize=(10,6))
sns.swarmplot(x='Features', y='Values', hue='blueWins', data=data)
plt.xticks(rotation=45)
plt.show()
```

可看出，击杀数、死亡数、助攻数和新构造的两个diff都有较好的区分度

再检查一下pairplot，看能不能发现新的区分度高的特征组合

```python
data = pd.concat([y, x], axis=1).sample(500)

sns.pairplot(data, vars=['blueKills','blueDeaths','blueAssists','killsDiff','assistsDiff','redAssists'], 
             hue='blueWins')
plt.show()
```



- 针对：龙和精英怪

```python
x['dragonsDiff'] = x['blueDragons'] - x['redDragons']
x['heraldsDiff'] = x['blueHeralds'] - x['redHeralds']
x['eliteDiff'] = x['blueEliteMonsters'] - x['redEliteMonsters']

data = pd.concat([y, x], axis=1)

eliteGroup = data.groupby(['eliteDiff'])['blueWins'].mean()
dragonGroup = data.groupby(['dragonsDiff'])['blueWins'].mean()
heraldGroup = data.groupby(['heraldsDiff'])['blueWins'].mean()

fig, ax = plt.subplots(1,3, figsize=(15,4))

eliteGroup.plot(kind='bar', ax=ax[0])
dragonGroup.plot(kind='bar', ax=ax[1])
heraldGroup.plot(kind='bar', ax=ax[2])

print(eliteGroup)
print(dragonGroup)
print(heraldGroup)

plt.show()
```

发现这3个都是强相关



- 针对：塔

```python
x['towerDiff'] = x['blueTowersDestroyed'] - x['redTowersDestroyed']

data = pd.concat([y, x], axis=1)

towerGroup = data.groupby(['towerDiff'])['blueWins']
print(towerGroup.count())
print(towerGroup.mean())

fig, ax = plt.subplots(1,2,figsize=(15,5))

towerGroup.mean().plot(kind='line', ax=ax[0])
ax[0].set_title('Proportion of Blue Wins')
ax[0].set_ylabel('Proportion')

towerGroup.count().plot(kind='line', ax=ax[1])
ax[1].set_title('Count of Towers Destroyed')
ax[1].set_ylabel('Count')
```

*这里使用了blueWins的平均值作为获胜概率

发现：大部分情况推塔数量都相差无几，但推塔差和胜利概率关系明显



## 5. 模型训练与预测

利用LightGBM

### 划分数据集

```python
## 为了正确评估模型性能，将数据划分为训练集和测试集，并在训练集上训练模型，在测试集上验证模型性能。
from sklearn.model_selection import train_test_split

## 选择其类别为0和1的样本 （不包括类别为2的样本）
data_target_part = y
data_features_part = x

## 测试集大小为20%， 80%/20%分
x_train, x_test, y_train, y_test = train_test_split(data_features_part, data_target_part, test_size = 0.2, random_state = 2020)
```

### 导入模型

```python
## 导入LightGBM模型
from lightgbm.sklearn import LGBMClassifier
## 定义 LightGBM 模型 
clf = LGBMClassifier()
# 在训练集上训练LightGBM模型
clf.fit(x_train, y_train)
```

### 预测，校验，可视化结果

```python
## 在训练集和测试集上分布利用训练好的模型进行预测
train_predict = clf.predict(x_train)
test_predict = clf.predict(x_test)
from sklearn import metrics

## 利用accuracy（准确度）【预测正确的样本数目占总预测样本数目的比例】评估模型效果
print('The accuracy of the Logistic Regression is:',metrics.accuracy_score(y_train,train_predict))
print('The accuracy of the Logistic Regression is:',metrics.accuracy_score(y_test,test_predict))

## 查看混淆矩阵 (预测值和真实值的各类情况统计矩阵)
confusion_matrix_result = metrics.confusion_matrix(test_predict,y_test)
print('The confusion matrix result:\n',confusion_matrix_result)

# 利用热力图对于结果进行可视化
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix_result, annot=True, cmap='Blues')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.show()
```



## 6. 特征选择

### 知识点

- LightGBM的特征选择属于特征选择中的嵌入式方法，在LightGBM中可以用属性feature_importances_去查看特征的重要度。
- LightGBM评估特征重要程度指标
  - clf.feature_importances_：LightGBM模型的特征重要度
  - gain：基尼指数
  - split：特征被用到的次数
- Sns.barplot

## 分析过程

```python
sns.barplot(y=data_features_part.columns, x=clf.feature_importances_)
```

可看出，经济差距，助攻数量，击杀死亡等特征都比较重要。

```python
from sklearn.metrics import accuracy_score
from lightgbm import plot_importance

def estimate(model,data):
    #sns.barplot(data.columns,model.feature_importances_)
    ax1=plot_importance(model,importance_type="gain")
    ax1.set_title('gain')
    ax2=plot_importance(model, importance_type="split")
    ax2.set_title('split')
    plt.show()
def classes(data,label,test):
    model=LGBMClassifier()
    model.fit(data,label)
    ans=model.predict(test)
    estimate(model, data)
    return ans
 
ans=classes(x_train,y_train,x_test)
pre=accuracy_score(y_test, ans)
print('acc=',accuracy_score(y_test,ans))
```



## 7. 调参

### 知识点

- sklearn.model_selection import GridSearchCV：网格调参
- Sklearn.metrics.acuracy_score(y_true, predict_y): 模型准确度
- ...metrics.confusion_matrix(test_predict, y_test)：混淆矩阵
- sns.heatmap()：热力图。用于可视化混淆矩阵

### 过程

```python
## 从sklearn库中导入网格调参函数
from sklearn.model_selection import GridSearchCV

## 定义参数取值范围
learning_rate = [0.1, 0.3, 0.6]
feature_fraction = [0.5, 0.8, 1]
num_leaves = [16, 32, 64]
max_depth = [-1,3,5,8]

parameters = { 'learning_rate': learning_rate,
              'feature_fraction':feature_fraction,
              'num_leaves': num_leaves,
              'max_depth': max_depth}
model = LGBMClassifier(n_estimators = 50)

## 进行网格搜索
clf = GridSearchCV(model, parameters, cv=3, scoring='accuracy',verbose=3, n_jobs=-1)
clf = clf.fit(x_train, y_train)
```

```python
## 网格搜索后的最好参数为
clf.best_params_
```

用新参数再次训练模型

```python
clf = LGBMClassifier(feature_fraction=0.8,
                     learning_rate=0.1, max_depth=-1, num_leaves=4)
clf.fit(x_train, y_train)

train_predict = clf.predict(x_train)
test_predict = clf.predict(x_test)

## 利用accuracy（准确度）【预测正确的样本数目占总预测样本数目的比例】评估模型效果
print('The accuracy of the Logistic Regression is:',metrics.accuracy_score(y_train,train_predict))
print('The accuracy of the Logistic Regression is:',metrics.accuracy_score(y_test,test_predict))

confusion_matrix_result = metrics.confusion_matrix(test_predict, y_test)
print('Confusion matrix result: \n', confusion_matrix_result)

plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix_result, annot=True, fmt='.20g', cmap='Blues')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.show()
```

对比调参前模型有显著提升



# LightGBM粗略原理

LightGBM底层实现了GBDT算法，并且添加了一系列的新特性：

1. 基于直方图算法进行优化，使数据存储更加方便、运算更快、鲁棒性强、模型更加稳定等。
2. 提出了带深度限制的 Leaf-wise 算法，抛弃了大多数GBDT工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长策略，可以降低误差，得到更好的精度。
3. 提出了单边梯度采样算法，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。
4. 提出了互斥特征捆绑算法，高维度的数据往往是稀疏的，这种稀疏性启发我们设计一种无损的方法来减少特征的维度。通常被捆绑的特征都是互斥的（即特征不会同时为非零值，像one-hot），这样两个特征捆绑起来就不会丢失信息。

LightGBM是基于CART树的集成模型，它的思想是串联多个决策树模型共同进行决策。

![image.png](data:image/png;base64,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)

那么如何串联呢？LightGBM采用迭代预测误差的方法串联。举个通俗的例子，我们现在需要预测一辆车价值3000元。我们构建决策树1训练后预测为2600元，我们发现有400元的误差，那么决策树2的训练目标为400元，但决策树2的预测结果为350元，还存在50元的误差就交给第三棵树……以此类推，每一颗树用来估计之前所有树的误差，最后所有树预测结果的求和就是最终预测结果！

LightGBM的基模型是CART回归树，它有两个特点：（1）CART树，是一颗二叉树。（2）回归树，最后拟合结果是连续值。

LightGBM模型可以表示为以下形式，我们约定𝑓𝑡(𝑥)ft(x)表示前𝑡t颗树的和，ℎ𝑡(𝑥)ht(x)表示第𝑡t颗决策树，模型定义如下：

𝑓𝑡(𝑥)=∑𝑇𝑡=1ℎ𝑡(𝑥)ft(x)=∑t=1Tht(x)

由于模型递归生成，第𝑡t步的模型由第𝑡−1t−1步的模型形成，可以写成：

𝑓𝑡(𝑥)=𝑓𝑡−1(𝑥)+ℎ𝑡(𝑥)ft(x)=ft−1(x)+ht(x)

每次需要加上的树ℎ𝑡(𝑥)ht(x)是之前树求和的误差：

𝑟𝑡,𝑖=𝑦𝑖−𝑓𝑚−1(𝑥𝑖)rt,i=yi−fm−1(xi)

我们每一步只要拟合一颗输出为𝑟𝑡,𝑖rt,i的CART树加到𝑓𝑡−1(𝑥)ft−1(x)就可以了。





